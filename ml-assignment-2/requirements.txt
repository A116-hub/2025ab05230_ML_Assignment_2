# ML Assignment 2 â€“ Classification Models

## a. Problem Statement

The objective of this assignment is to implement and evaluate multiple machine learning
classification models on a healthcare dataset. The project also demonstrates deployment
of the trained models using a Streamlit web application.

---

## b. Dataset Description

The healthcare dataset contains patient-related attributes used to predict health outcomes.
The dataset satisfies the assignment requirements with more than 500 instances and more
than 12 features. The target variable is a multi-class label representing different outcome
categories.

---

## c. Models Used and Evaluation Metrics

The following six classification models were implemented and evaluated using the same
dataset and train-test split:

1. Logistic Regression  
2. Decision Tree Classifier  
3. K-Nearest Neighbor (kNN)  
4. Naive Bayes (Gaussian)  
5. Random Forest (Ensemble)  
6. XGBoost (Ensemble)  

The evaluation metrics used are:
- Accuracy
- AUC Score
- Precision
- Recall
- F1 Score
- Matthews Correlation Coefficient (MCC)

---

### Model Performance Comparison Table

| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |
|--------------|----------|-----|-----------|--------|----|-----|
| Logistic Regression | XX | XX | XX | XX | XX | XX |
| Decision Tree | XX | XX | XX | XX | XX | XX |
| kNN | XX | XX | XX | XX | XX | XX |
| Naive Bayes | XX | XX | XX | XX | XX | XX |
| Random Forest (Ensemble) | XX | XX | XX | XX | XX | XX |
| XGBoost (Ensemble) | XX | XX | XX | XX | XX | XX |

(Note: Replace XX with actual values obtained from model evaluation.)

---

## d. Observations on Model Performance

| ML Model Name | Observation about Model Performance |
|--------------|-------------------------------------|
| Logistic Regression | Provided a strong baseline performance with stable and consistent results across evaluation metrics. |
| Decision Tree | Showed good performance but exhibited signs of overfitting compared to ensemble models. |
| kNN | Performance was sensitive to feature scaling and choice of k value. |
| Naive Bayes | Fast and computationally efficient but less expressive due to strong independence assumptions. |
| Random Forest (Ensemble) | Achieved strong performance by reducing overfitting through ensemble learning. |
| XGBoost (Ensemble) | Delivered the best overall performance with high accuracy and AUC due to gradient boosting. |

---

## Deployment

The trained models are deployed using Streamlit Community Cloud. The application allows
users to upload test data, select a model, and view predictions and evaluation metrics.
